{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "I'll do a baseline model performance first, so that we can see whether the feature engineering method is effective and useful or not\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model\n",
    "\n",
    "I'll compare using some of the mainstream models, like\n",
    "\n",
    "- DummyClassifier (can be most-frequent class, or by random (stratified and unstratified))\n",
    "- Logistic Regression (since target variable is a boolean)\n",
    "- Decision Tree Classifier\n",
    "- Naive Bayes\n",
    "- Random Forest Classifier\n",
    "- XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Administrative</th>\n",
       "      <th>Administrative_Duration</th>\n",
       "      <th>Informational</th>\n",
       "      <th>Informational_Duration</th>\n",
       "      <th>ProductRelated</th>\n",
       "      <th>ProductRelated_Duration</th>\n",
       "      <th>BounceRates</th>\n",
       "      <th>ExitRates</th>\n",
       "      <th>PageValues</th>\n",
       "      <th>SpecialDay</th>\n",
       "      <th>Month</th>\n",
       "      <th>OperatingSystems</th>\n",
       "      <th>Browser</th>\n",
       "      <th>Region</th>\n",
       "      <th>TrafficType</th>\n",
       "      <th>VisitorType</th>\n",
       "      <th>Weekend</th>\n",
       "      <th>Revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feb</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feb</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feb</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feb</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>627.500000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feb</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12325</th>\n",
       "      <td>3</td>\n",
       "      <td>145.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53</td>\n",
       "      <td>1783.791667</td>\n",
       "      <td>0.007143</td>\n",
       "      <td>0.029031</td>\n",
       "      <td>12.241717</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Dec</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12326</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>465.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Nov</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12327</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>184.250000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.086667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Nov</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12328</th>\n",
       "      <td>4</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "      <td>346.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021053</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Nov</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12329</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>21.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Nov</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>New_Visitor</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12330 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Administrative  Administrative_Duration  Informational  \\\n",
       "0                   0                      0.0              0   \n",
       "1                   0                      0.0              0   \n",
       "2                   0                      0.0              0   \n",
       "3                   0                      0.0              0   \n",
       "4                   0                      0.0              0   \n",
       "...               ...                      ...            ...   \n",
       "12325               3                    145.0              0   \n",
       "12326               0                      0.0              0   \n",
       "12327               0                      0.0              0   \n",
       "12328               4                     75.0              0   \n",
       "12329               0                      0.0              0   \n",
       "\n",
       "       Informational_Duration  ProductRelated  ProductRelated_Duration  \\\n",
       "0                         0.0               1                 0.000000   \n",
       "1                         0.0               2                64.000000   \n",
       "2                         0.0               1                 0.000000   \n",
       "3                         0.0               2                 2.666667   \n",
       "4                         0.0              10               627.500000   \n",
       "...                       ...             ...                      ...   \n",
       "12325                     0.0              53              1783.791667   \n",
       "12326                     0.0               5               465.750000   \n",
       "12327                     0.0               6               184.250000   \n",
       "12328                     0.0              15               346.000000   \n",
       "12329                     0.0               3                21.250000   \n",
       "\n",
       "       BounceRates  ExitRates  PageValues  SpecialDay Month  OperatingSystems  \\\n",
       "0         0.200000   0.200000    0.000000         0.0   Feb                 1   \n",
       "1         0.000000   0.100000    0.000000         0.0   Feb                 2   \n",
       "2         0.200000   0.200000    0.000000         0.0   Feb                 4   \n",
       "3         0.050000   0.140000    0.000000         0.0   Feb                 3   \n",
       "4         0.020000   0.050000    0.000000         0.0   Feb                 3   \n",
       "...            ...        ...         ...         ...   ...               ...   \n",
       "12325     0.007143   0.029031   12.241717         0.0   Dec                 4   \n",
       "12326     0.000000   0.021333    0.000000         0.0   Nov                 3   \n",
       "12327     0.083333   0.086667    0.000000         0.0   Nov                 3   \n",
       "12328     0.000000   0.021053    0.000000         0.0   Nov                 2   \n",
       "12329     0.000000   0.066667    0.000000         0.0   Nov                 3   \n",
       "\n",
       "       Browser  Region  TrafficType        VisitorType  Weekend  Revenue  \n",
       "0            1       1            1  Returning_Visitor    False    False  \n",
       "1            2       1            2  Returning_Visitor    False    False  \n",
       "2            1       9            3  Returning_Visitor    False    False  \n",
       "3            2       2            4  Returning_Visitor    False    False  \n",
       "4            3       1            4  Returning_Visitor     True    False  \n",
       "...        ...     ...          ...                ...      ...      ...  \n",
       "12325        6       1            1  Returning_Visitor     True    False  \n",
       "12326        2       1            8  Returning_Visitor     True    False  \n",
       "12327        2       1           13  Returning_Visitor     True    False  \n",
       "12328        2       3           11  Returning_Visitor    False    False  \n",
       "12329        2       1            2        New_Visitor     True    False  \n",
       "\n",
       "[12330 rows x 18 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"../data/raw/online_shoppers_intention.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Dummy (most frequent)\n",
      "Accuracy: 0.8333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.83      1.00      0.91      2055\n",
      "        True       0.00      0.00      0.00       411\n",
      "\n",
      "    accuracy                           0.83      2466\n",
      "   macro avg       0.42      0.50      0.45      2466\n",
      "weighted avg       0.69      0.83      0.76      2466\n",
      "\n",
      "--------------------------------------------------\n",
      "Model: Dummy (stratified)\n",
      "Accuracy: 0.7332\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.84      0.85      0.84      2055\n",
      "        True       0.18      0.17      0.17       411\n",
      "\n",
      "    accuracy                           0.73      2466\n",
      "   macro avg       0.51      0.51      0.51      2466\n",
      "weighted avg       0.73      0.73      0.73      2466\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enochmok/miniconda3/envs/data/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/enochmok/miniconda3/envs/data/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/enochmok/miniconda3/envs/data/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/enochmok/miniconda3/envs/data/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic Regression\n",
      "Accuracy: 0.8715\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.88      0.98      0.93      2055\n",
      "        True       0.74      0.35      0.47       411\n",
      "\n",
      "    accuracy                           0.87      2466\n",
      "   macro avg       0.81      0.66      0.70      2466\n",
      "weighted avg       0.86      0.87      0.85      2466\n",
      "\n",
      "--------------------------------------------------\n",
      "Model: Decision Tree\n",
      "Accuracy: 0.8573\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.92      0.91      0.91      2055\n",
      "        True       0.57      0.59      0.58       411\n",
      "\n",
      "    accuracy                           0.86      2466\n",
      "   macro avg       0.74      0.75      0.75      2466\n",
      "weighted avg       0.86      0.86      0.86      2466\n",
      "\n",
      "--------------------------------------------------\n",
      "Model: Naive Bayes\n",
      "Accuracy: 0.8021\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.92      0.84      0.88      2055\n",
      "        True       0.43      0.62      0.51       411\n",
      "\n",
      "    accuracy                           0.80      2466\n",
      "   macro avg       0.68      0.73      0.69      2466\n",
      "weighted avg       0.84      0.80      0.81      2466\n",
      "\n",
      "--------------------------------------------------\n",
      "Model: Random Forest\n",
      "Accuracy: 0.8954\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.92      0.96      0.94      2055\n",
      "        True       0.75      0.55      0.64       411\n",
      "\n",
      "    accuracy                           0.90      2466\n",
      "   macro avg       0.83      0.76      0.79      2466\n",
      "weighted avg       0.89      0.90      0.89      2466\n",
      "\n",
      "--------------------------------------------------\n",
      "Model: XGBoost\n",
      "Accuracy: 0.8917\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.92      0.96      0.94      2055\n",
      "        True       0.72      0.57      0.64       411\n",
      "\n",
      "    accuracy                           0.89      2466\n",
      "   macro avg       0.82      0.76      0.79      2466\n",
      "weighted avg       0.89      0.89      0.89      2466\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Splitting the data into features and target\n",
    "X = data.drop(columns=['Revenue'])\n",
    "y = data['Revenue']\n",
    "\n",
    "# One-hot encoding for categorical variables\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Splitting into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Models to evaluate\n",
    "models = {\n",
    "    \"Dummy (most frequent)\": DummyClassifier(strategy=\"most_frequent\"),\n",
    "    \"Dummy (stratified)\": DummyClassifier(strategy=\"stratified\"),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"XGBoost\": XGBClassifier(eval_metric='logloss')\n",
    "}\n",
    "\n",
    "# Evaluating each model\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"Model: {name}\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model                 | Accuracy | Precision (False) | Recall (False) | F1-Score (False) | Precision (True) | Recall (True) | F1-Score (True) | Macro Avg Precision | Macro Avg Recall | Macro Avg F1-Score | Weighted Avg Precision | Weighted Avg Recall | Weighted Avg F1-Score |\n",
    "| --------------------- | -------- | ----------------- | -------------- | ---------------- | ---------------- | ------------- | --------------- | ------------------- | ---------------- | ------------------ | ---------------------- | ------------------- | --------------------- |\n",
    "| Dummy (most frequent) | 0.8333   | 0.83              | 1.00           | 0.91             | 0.00             | 0.00          | 0.00            | 0.42                | 0.50             | 0.45               | 0.69                   | 0.83                | 0.76                  |\n",
    "| Dummy (stratified)    | 0.7332   | 0.84              | 0.85           | 0.84             | 0.18             | 0.17          | 0.17            | 0.51                | 0.51             | 0.51               | 0.73                   | 0.73                | 0.73                  |\n",
    "| Logistic Regression   | 0.8715   | 0.88              | 0.98           | 0.93             | 0.74             | 0.35          | 0.47            | 0.81                | 0.66             | 0.70               | 0.86                   | 0.87                | 0.85                  |\n",
    "| Decision Tree         | 0.8573   | 0.92              | 0.91           | 0.91             | 0.57             | 0.59          | 0.58            | 0.74                | 0.75             | 0.75               | 0.86                   | 0.86                | 0.86                  |\n",
    "| Naive Bayes           | 0.8021   | 0.92              | 0.84           | 0.88             | 0.43             | 0.62          | 0.51            | 0.68                | 0.73             | 0.69               | 0.84                   | 0.80                | 0.81                  |\n",
    "| Random Forest         | 0.8954   | 0.92              | 0.96           | 0.94             | 0.75             | 0.55          | 0.64            | 0.83                | 0.76             | 0.79               | 0.89                   | 0.90                | 0.89                  |\n",
    "| XGBoost               | 0.8917   | 0.92              | 0.96           | 0.94             | 0.72             | 0.57          | 0.64            | 0.82                | 0.76             | 0.79               | 0.89                   | 0.89                | 0.89                  |\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Random Forest** had the best accuracy, with 0.8954 (not what I expected, as I always thought XGBoost was like the 'best' defacto model in the market right now)\n",
    "\n",
    "One more thing to note is that the dataset doesn't have alot of positive samples (15.5%), as such looking into metrics like Precision and Recall is important.\n",
    "Precision ensures false positives are reduced, while recall ensures false negatives are reduced. In our case, since we are predicting customer's conversion from visiting to an actual transaction, it is more important to reduce FPs.\n",
    "\n",
    "Hence Precision is slightly more important in this case, and Random Forest also has the highest precision among all the models tested\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible Ways of Performing Feature Engineering\n",
    "\n",
    "(Generated from GPT, but will need to vet through and see if they are applicable)\n",
    "\n",
    "1. **Handling Missing Values**:\n",
    "\n",
    "   - Impute missing numerical values with mean, median, or a constant.\n",
    "   - Impute missing categorical values with the most frequent value or a constant.\n",
    "\n",
    "> Enoch's comment: No missing values\n",
    "\n",
    "2. **Scaling and Normalization**:\n",
    "\n",
    "   - Standardize numerical features using StandardScaler.\n",
    "   - Normalize numerical features using MinMaxScaler or RobustScaler.\n",
    "\n",
    "> Enoch's comment: Makes sense, helps process data for ML training later on\n",
    "\n",
    "3. **Encoding Categorical Variables**:\n",
    "\n",
    "   - One-hot encoding for nominal categorical variables.\n",
    "   - Ordinal encoding for ordinal categorical variables.\n",
    "\n",
    "> Enoch's comment: Ordinal: 'Month' ; Nominal: 'OperatingSystem', 'Browser', 'Region', 'TrafficType', 'VisitorType'\n",
    "\n",
    "4. **Interaction Features**:\n",
    "\n",
    "   - Create interaction terms between numerical features (e.g., `Administrative * Administrative_Duration`).\n",
    "   - Create interaction terms between categorical features (e.g., `Month * VisitorType`).\n",
    "\n",
    "> Enoch's comment: How do I figure out which features are 'good' interaction terms?\n",
    "\n",
    "5. **Polynomial Features**:\n",
    "\n",
    "   - Generate polynomial features for numerical columns (e.g., square, cube).\n",
    "\n",
    "> Enoch's comment: How do I figure out which features should use polynomial features?\n",
    "\n",
    "6. **Feature Binning**:\n",
    "\n",
    "   - Bin numerical features into discrete intervals (e.g., binning `Administrative_Duration` into low, medium, high).\n",
    "\n",
    "> Enoch's comment: Columns that are applicable: 'Administrative', 'Administrative_Duration', 'Informational', 'Informational_Duration', 'ProductRelated', 'ProductRelated_Duration'. Maybe 'PageValues' too? IDK\n",
    "\n",
    "7. **Log Transformation**:\n",
    "\n",
    "   - Apply log transformation to skewed numerical features (e.g., `np.log1p(ProductRelated_Duration)`).\n",
    "\n",
    "> Enoch's comment: How to find skewed numerical features?\n",
    "\n",
    "8. **Feature Aggregation**:\n",
    "\n",
    "   - Aggregate related features (e.g., sum or mean of `Administrative`, `Informational`, and `ProductRelated`).\n",
    "\n",
    "> Enoch's comment: This makes sense - probably Sum or Mean makes the most sense. Not sure if min/max will also play a similar effect. Maybe a short maximum time spent also has a correlation to whether the user decides to proceed with a transaction\n",
    "\n",
    "9. **Feature Selection**:\n",
    "\n",
    "   - Use statistical tests (e.g., ANOVA, chi-square) to select important features.\n",
    "   - Use model-based feature selection (e.g., feature importance from tree-based models).\n",
    "\n",
    "> Enoch's comment: Can try this\n",
    "\n",
    "10. **Dimensionality Reduction**:\n",
    "\n",
    "    - Apply PCA (Principal Component Analysis) to reduce dimensionality of numerical features.\n",
    "    - Use t-SNE or UMAP for visualization or dimensionality reduction.\n",
    "\n",
    "> Enoch's comment: Not very familiar with this - have heard about it, and know what it does, but not sure how it will affect or make the data 'better'\n",
    "\n",
    "11. **Temporal Features**:\n",
    "\n",
    "    - Extract temporal features from `Month` (e.g., seasonality, quarter).\n",
    "    - Create binary features for specific time periods (e.g., `is_holiday`).\n",
    "\n",
    "> Enoch's comment: Hmm... need to explore more into this\n",
    "\n",
    "12. **Target Encoding**:\n",
    "\n",
    "    - Encode categorical variables based on their relationship with the target variable (e.g., mean encoding).\n",
    "\n",
    "> Enoch's comment: Huh?\n",
    "\n",
    "13. **Text Features**:\n",
    "\n",
    "    - If any text data exists, extract features using TF-IDF or word embeddings.\n",
    "\n",
    "> Enoch's comment: Don't have\n",
    "\n",
    "14. **Outlier Detection and Removal**:\n",
    "\n",
    "    - Detect and remove outliers using statistical methods (e.g., z-score, IQR).\n",
    "\n",
    "> Enoch's comment: Probably applicable to the numerical ones\n",
    "\n",
    "15. **Feature Clustering**:\n",
    "\n",
    "    - Cluster similar features and create cluster labels as new features.\n",
    "\n",
    "> Enoch's comment: ?\n",
    "\n",
    "16. **Feature Importance Analysis**:\n",
    "\n",
    "    - Analyze feature importance using models like Random Forest or XGBoost and drop less important features.\n",
    "\n",
    "> Enoch's comment: ??\n",
    "\n",
    "17. **Custom Feature Engineering**:\n",
    "\n",
    "    - Create domain-specific features (e.g., `BounceRates / ExitRates` to measure engagement).\n",
    "\n",
    "> Enoch's comment: ???\n",
    "\n",
    "18. **Binary Features**:\n",
    "\n",
    "    - Convert categorical features into binary flags (e.g., `Weekend` as 0 or 1).\n",
    "\n",
    "> Enoch's comment: Easy - just convert boolean to int\n",
    "\n",
    "19. **Interaction with Target**:\n",
    "\n",
    "    - Create features based on interaction with the target variable (e.g., conditional probabilities).\n",
    "\n",
    "> Enoch's comment: Huh...?\n",
    "\n",
    "20. **Cross-Validation Based Features**:\n",
    "\n",
    "    - Generate features using cross-validation predictions (e.g., stacking or blending).\n",
    "\n",
    "> Enoch's comment:\n",
    "\n",
    "21. **Feature Hashing**:\n",
    "\n",
    "    - Use feature hashing for high-cardinality categorical variables.\n",
    "\n",
    "> Enoch's comment:\n",
    "\n",
    "22. **Lag Features**:\n",
    "\n",
    "    - Create lag features for time-series data (e.g., previous month's `PageValues`).\n",
    "\n",
    "> Enoch's comment:\n",
    "\n",
    "23. **Cyclic Features**:\n",
    "\n",
    "    - Encode cyclic features like `Month` using sine and cosine transformations.\n",
    "\n",
    "> Enoch's comment: I heard about this, can look more into this\n",
    "\n",
    "24. **Feature Pruning**:\n",
    "\n",
    "    - Remove highly correlated features to avoid multicollinearity.\n",
    "\n",
    "> Enoch's comment:\n",
    "\n",
    "25. **Synthetic Features**:\n",
    "    - Generate synthetic features using GANs or other generative models.\n",
    "\n",
    "> Enoch's comment: Something to consider in the future, considering the number of positive samples are quite low (15.5%)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
