{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "I'll do a baseline model performance first, so that we can see whether the feature engineering method is effective and useful or not\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model\n",
    "\n",
    "I'll compare using some of the mainstream models, like\n",
    "\n",
    "- DummyClassifier (can be most-frequent class, or by random (stratified and unstratified))\n",
    "- Logistic Regression (since target variable is a boolean)\n",
    "- Decision Tree Classifier\n",
    "- Naive Bayes\n",
    "- Random Forest Classifier\n",
    "- XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Administrative</th>\n",
       "      <th>Administrative_Duration</th>\n",
       "      <th>Informational</th>\n",
       "      <th>Informational_Duration</th>\n",
       "      <th>ProductRelated</th>\n",
       "      <th>ProductRelated_Duration</th>\n",
       "      <th>BounceRates</th>\n",
       "      <th>ExitRates</th>\n",
       "      <th>PageValues</th>\n",
       "      <th>SpecialDay</th>\n",
       "      <th>Month</th>\n",
       "      <th>OperatingSystems</th>\n",
       "      <th>Browser</th>\n",
       "      <th>Region</th>\n",
       "      <th>TrafficType</th>\n",
       "      <th>VisitorType</th>\n",
       "      <th>Weekend</th>\n",
       "      <th>Revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feb</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feb</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feb</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feb</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>627.500000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feb</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12325</th>\n",
       "      <td>3</td>\n",
       "      <td>145.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53</td>\n",
       "      <td>1783.791667</td>\n",
       "      <td>0.007143</td>\n",
       "      <td>0.029031</td>\n",
       "      <td>12.241717</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Dec</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12326</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>465.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Nov</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12327</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>184.250000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.086667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Nov</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12328</th>\n",
       "      <td>4</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "      <td>346.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021053</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Nov</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12329</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>21.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Nov</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>New_Visitor</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12330 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Administrative  Administrative_Duration  Informational  \\\n",
       "0                   0                      0.0              0   \n",
       "1                   0                      0.0              0   \n",
       "2                   0                      0.0              0   \n",
       "3                   0                      0.0              0   \n",
       "4                   0                      0.0              0   \n",
       "...               ...                      ...            ...   \n",
       "12325               3                    145.0              0   \n",
       "12326               0                      0.0              0   \n",
       "12327               0                      0.0              0   \n",
       "12328               4                     75.0              0   \n",
       "12329               0                      0.0              0   \n",
       "\n",
       "       Informational_Duration  ProductRelated  ProductRelated_Duration  \\\n",
       "0                         0.0               1                 0.000000   \n",
       "1                         0.0               2                64.000000   \n",
       "2                         0.0               1                 0.000000   \n",
       "3                         0.0               2                 2.666667   \n",
       "4                         0.0              10               627.500000   \n",
       "...                       ...             ...                      ...   \n",
       "12325                     0.0              53              1783.791667   \n",
       "12326                     0.0               5               465.750000   \n",
       "12327                     0.0               6               184.250000   \n",
       "12328                     0.0              15               346.000000   \n",
       "12329                     0.0               3                21.250000   \n",
       "\n",
       "       BounceRates  ExitRates  PageValues  SpecialDay Month  OperatingSystems  \\\n",
       "0         0.200000   0.200000    0.000000         0.0   Feb                 1   \n",
       "1         0.000000   0.100000    0.000000         0.0   Feb                 2   \n",
       "2         0.200000   0.200000    0.000000         0.0   Feb                 4   \n",
       "3         0.050000   0.140000    0.000000         0.0   Feb                 3   \n",
       "4         0.020000   0.050000    0.000000         0.0   Feb                 3   \n",
       "...            ...        ...         ...         ...   ...               ...   \n",
       "12325     0.007143   0.029031   12.241717         0.0   Dec                 4   \n",
       "12326     0.000000   0.021333    0.000000         0.0   Nov                 3   \n",
       "12327     0.083333   0.086667    0.000000         0.0   Nov                 3   \n",
       "12328     0.000000   0.021053    0.000000         0.0   Nov                 2   \n",
       "12329     0.000000   0.066667    0.000000         0.0   Nov                 3   \n",
       "\n",
       "       Browser  Region  TrafficType        VisitorType  Weekend  Revenue  \n",
       "0            1       1            1  Returning_Visitor    False    False  \n",
       "1            2       1            2  Returning_Visitor    False    False  \n",
       "2            1       9            3  Returning_Visitor    False    False  \n",
       "3            2       2            4  Returning_Visitor    False    False  \n",
       "4            3       1            4  Returning_Visitor     True    False  \n",
       "...        ...     ...          ...                ...      ...      ...  \n",
       "12325        6       1            1  Returning_Visitor     True    False  \n",
       "12326        2       1            8  Returning_Visitor     True    False  \n",
       "12327        2       1           13  Returning_Visitor     True    False  \n",
       "12328        2       3           11  Returning_Visitor    False    False  \n",
       "12329        2       1            2        New_Visitor     True    False  \n",
       "\n",
       "[12330 rows x 18 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"../data/raw/online_shoppers_intention.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Dummy (most frequent)\n",
      "Accuracy: 0.8333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.83      1.00      0.91      2055\n",
      "        True       0.00      0.00      0.00       411\n",
      "\n",
      "    accuracy                           0.83      2466\n",
      "   macro avg       0.42      0.50      0.45      2466\n",
      "weighted avg       0.69      0.83      0.76      2466\n",
      "\n",
      "--------------------------------------------------\n",
      "Model: Dummy (stratified)\n",
      "Accuracy: 0.7336\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.83      0.86      0.84      2055\n",
      "        True       0.14      0.12      0.13       411\n",
      "\n",
      "    accuracy                           0.73      2466\n",
      "   macro avg       0.48      0.49      0.49      2466\n",
      "weighted avg       0.71      0.73      0.72      2466\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enochmok/miniconda3/envs/data/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/enochmok/miniconda3/envs/data/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/enochmok/miniconda3/envs/data/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/enochmok/miniconda3/envs/data/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic Regression\n",
      "Accuracy: 0.8698\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.88      0.98      0.93      2055\n",
      "        True       0.74      0.33      0.46       411\n",
      "\n",
      "    accuracy                           0.87      2466\n",
      "   macro avg       0.81      0.66      0.69      2466\n",
      "weighted avg       0.86      0.87      0.85      2466\n",
      "\n",
      "--------------------------------------------------\n",
      "Model: Decision Tree\n",
      "Accuracy: 0.8556\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.91      0.91      0.91      2055\n",
      "        True       0.57      0.57      0.57       411\n",
      "\n",
      "    accuracy                           0.86      2466\n",
      "   macro avg       0.74      0.74      0.74      2466\n",
      "weighted avg       0.86      0.86      0.86      2466\n",
      "\n",
      "--------------------------------------------------\n",
      "Model: Naive Bayes\n",
      "Accuracy: 0.7891\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.92      0.82      0.87      2055\n",
      "        True       0.41      0.64      0.50       411\n",
      "\n",
      "    accuracy                           0.79      2466\n",
      "   macro avg       0.67      0.73      0.69      2466\n",
      "weighted avg       0.84      0.79      0.81      2466\n",
      "\n",
      "--------------------------------------------------\n",
      "Model: Random Forest\n",
      "Accuracy: 0.8909\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.91      0.96      0.94      2055\n",
      "        True       0.73      0.54      0.62       411\n",
      "\n",
      "    accuracy                           0.89      2466\n",
      "   macro avg       0.82      0.75      0.78      2466\n",
      "weighted avg       0.88      0.89      0.88      2466\n",
      "\n",
      "--------------------------------------------------\n",
      "Model: XGBoost\n",
      "Accuracy: 0.8946\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.92      0.96      0.94      2055\n",
      "        True       0.73      0.58      0.65       411\n",
      "\n",
      "    accuracy                           0.89      2466\n",
      "   macro avg       0.82      0.77      0.79      2466\n",
      "weighted avg       0.89      0.89      0.89      2466\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Splitting the data into features and target\n",
    "X = data.drop(columns=['Revenue'])\n",
    "y = data['Revenue']\n",
    "\n",
    "# One-hot encoding for categorical variables\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "# Splitting into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Models to evaluate\n",
    "models = {\n",
    "    \"Dummy (most frequent)\": DummyClassifier(strategy=\"most_frequent\"),\n",
    "    \"Dummy (stratified)\": DummyClassifier(strategy=\"stratified\"),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"XGBoost\": XGBClassifier(eval_metric='logloss')\n",
    "}\n",
    "\n",
    "# Evaluating each model\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"Model: {name}\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model                 | Accuracy | Precision (False) | Recall (False) | F1-Score (False) | Precision (True) | Recall (True) | F1-Score (True) | Macro Avg Precision | Macro Avg Recall | Macro Avg F1-Score | Weighted Avg Precision | Weighted Avg Recall | Weighted Avg F1-Score |\n",
    "| --------------------- | -------- | ----------------- | -------------- | ---------------- | ---------------- | ------------- | --------------- | ------------------- | ---------------- | ------------------ | ---------------------- | ------------------- | --------------------- |\n",
    "| Dummy (most frequent) | 0.8333   | 0.83              | 1.00           | 0.91             | 0.00             | 0.00          | 0.00            | 0.42                | 0.50             | 0.45               | 0.69                   | 0.83                | 0.76                  |\n",
    "| Dummy (stratified)    | 0.7332   | 0.84              | 0.85           | 0.84             | 0.18             | 0.17          | 0.17            | 0.51                | 0.51             | 0.51               | 0.73                   | 0.73                | 0.73                  |\n",
    "| Logistic Regression   | 0.8715   | 0.88              | 0.98           | 0.93             | 0.74             | 0.35          | 0.47            | 0.81                | 0.66             | 0.70               | 0.86                   | 0.87                | 0.85                  |\n",
    "| Decision Tree         | 0.8573   | 0.92              | 0.91           | 0.91             | 0.57             | 0.59          | 0.58            | 0.74                | 0.75             | 0.75               | 0.86                   | 0.86                | 0.86                  |\n",
    "| Naive Bayes           | 0.8021   | 0.92              | 0.84           | 0.88             | 0.43             | 0.62          | 0.51            | 0.68                | 0.73             | 0.69               | 0.84                   | 0.80                | 0.81                  |\n",
    "| Random Forest         | 0.8954   | 0.92              | 0.96           | 0.94             | 0.75             | 0.55          | 0.64            | 0.83                | 0.76             | 0.79               | 0.89                   | 0.90                | 0.89                  |\n",
    "| XGBoost               | 0.8917   | 0.92              | 0.96           | 0.94             | 0.72             | 0.57          | 0.64            | 0.82                | 0.76             | 0.79               | 0.89                   | 0.89                | 0.89                  |\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Random Forest** had the best accuracy, with 0.8954 (not what I expected, as I always thought XGBoost was like the 'best' defacto model in the market right now)\n",
    "\n",
    "One more thing to note is that the dataset doesn't have alot of positive samples (15.5%), as such looking into metrics like Precision and Recall is important.\n",
    "Precision ensures false positives are reduced, while recall ensures false negatives are reduced. In our case, since we are predicting customer's conversion from visiting to an actual transaction, it is more important to reduce FPs.\n",
    "\n",
    "Hence Precision is slightly more important in this case, and Random Forest also has the highest precision among all the models tested\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible Ways of Performing Feature Engineering\n",
    "\n",
    "(Generated from GPT, but will need to vet through and see if they are applicable)\n",
    "\n",
    "1. **Handling Missing Values**:\n",
    "\n",
    "   - Impute missing numerical values with mean, median, or a constant.\n",
    "   - Impute missing categorical values with the most frequent value or a constant.\n",
    "\n",
    "> Enoch's comment: No missing values\n",
    "\n",
    "2. **Scaling and Normalization**:\n",
    "\n",
    "   - Standardize numerical features using StandardScaler.\n",
    "   - Normalize numerical features using MinMaxScaler or RobustScaler.\n",
    "\n",
    "> Enoch's comment: Makes sense, helps process data for ML training later on\n",
    "\n",
    "3. **Encoding Categorical Variables**:\n",
    "\n",
    "   - One-hot encoding for nominal categorical variables.\n",
    "   - Ordinal encoding for ordinal categorical variables.\n",
    "\n",
    "> Enoch's comment: Ordinal: 'Month' ; Nominal: 'OperatingSystem', 'Browser', 'Region', 'TrafficType', 'VisitorType'\n",
    "\n",
    "4. **Interaction Features**:\n",
    "\n",
    "   - Create interaction terms between numerical features (e.g., `Administrative * Administrative_Duration`).\n",
    "   - Create interaction terms between categorical features (e.g., `Month * VisitorType`).\n",
    "\n",
    "> Enoch's comment: How do I figure out which features are 'good' interaction terms?\n",
    "\n",
    "5. **Polynomial Features**:\n",
    "\n",
    "   - Generate polynomial features for numerical columns (e.g., square, cube).\n",
    "\n",
    "> Enoch's comment: How do I figure out which features should use polynomial features?\n",
    "\n",
    "6. **Feature Binning**:\n",
    "\n",
    "   - Bin numerical features into discrete intervals (e.g., binning `Administrative_Duration` into low, medium, high).\n",
    "\n",
    "> Enoch's comment: Columns that are applicable: 'Administrative', 'Administrative_Duration', 'Informational', 'Informational_Duration', 'ProductRelated', 'ProductRelated_Duration'. Maybe 'PageValues' too? IDK\n",
    "\n",
    "7. **Log Transformation**:\n",
    "\n",
    "   - Apply log transformation to skewed numerical features (e.g., `np.log1p(ProductRelated_Duration)`).\n",
    "\n",
    "> Enoch's comment: How to find skewed numerical features?\n",
    "\n",
    "8. **Feature Aggregation**:\n",
    "\n",
    "   - Aggregate related features (e.g., sum or mean of `Administrative`, `Informational`, and `ProductRelated`).\n",
    "\n",
    "> Enoch's comment: This makes sense - probably Sum or Mean makes the most sense. Not sure if min/max will also play a similar effect. Maybe a short maximum time spent also has a correlation to whether the user decides to proceed with a transaction\n",
    "\n",
    "9. **Feature Selection**:\n",
    "\n",
    "   - Use statistical tests (e.g., ANOVA, chi-square) to select important features.\n",
    "   - Use model-based feature selection (e.g., feature importance from tree-based models).\n",
    "\n",
    "> Enoch's comment: Can try this\n",
    "\n",
    "10. **Dimensionality Reduction**:\n",
    "\n",
    "    - Apply PCA (Principal Component Analysis) to reduce dimensionality of numerical features.\n",
    "    - Use t-SNE or UMAP for visualization or dimensionality reduction.\n",
    "\n",
    "> Enoch's comment: Not very familiar with this - have heard about it, and know what it does, but not sure how it will affect or make the data 'better'\n",
    "\n",
    "11. **Temporal Features**:\n",
    "\n",
    "    - Extract temporal features from `Month` (e.g., seasonality, quarter).\n",
    "    - Create binary features for specific time periods (e.g., `is_holiday`).\n",
    "\n",
    "> Enoch's comment: Hmm... need to explore more into this\n",
    "\n",
    "12. **Target Encoding**:\n",
    "\n",
    "    - Encode categorical variables based on their relationship with the target variable (e.g., mean encoding).\n",
    "\n",
    "> Enoch's comment: Huh?\n",
    "\n",
    "13. **Text Features**:\n",
    "\n",
    "    - If any text data exists, extract features using TF-IDF or word embeddings.\n",
    "\n",
    "> Enoch's comment: Don't have\n",
    "\n",
    "14. **Outlier Detection and Removal**:\n",
    "\n",
    "    - Detect and remove outliers using statistical methods (e.g., z-score, IQR).\n",
    "\n",
    "> Enoch's comment: Probably applicable to the numerical ones\n",
    "\n",
    "15. **Feature Clustering**:\n",
    "\n",
    "    - Cluster similar features and create cluster labels as new features.\n",
    "\n",
    "> Enoch's comment: ?\n",
    "\n",
    "16. **Feature Importance Analysis**:\n",
    "\n",
    "    - Analyze feature importance using models like Random Forest or XGBoost and drop less important features.\n",
    "\n",
    "> Enoch's comment: ??\n",
    "\n",
    "17. **Custom Feature Engineering**:\n",
    "\n",
    "    - Create domain-specific features (e.g., `BounceRates / ExitRates` to measure engagement).\n",
    "\n",
    "> Enoch's comment: ???\n",
    "\n",
    "18. **Binary Features**:\n",
    "\n",
    "    - Convert categorical features into binary flags (e.g., `Weekend` as 0 or 1).\n",
    "\n",
    "> Enoch's comment: Easy - just convert boolean to int\n",
    "\n",
    "19. **Interaction with Target**:\n",
    "\n",
    "    - Create features based on interaction with the target variable (e.g., conditional probabilities).\n",
    "\n",
    "> Enoch's comment: Huh...?\n",
    "\n",
    "20. **Cross-Validation Based Features**:\n",
    "\n",
    "    - Generate features using cross-validation predictions (e.g., stacking or blending).\n",
    "\n",
    "> Enoch's comment:\n",
    "\n",
    "21. **Feature Hashing**:\n",
    "\n",
    "    - Use feature hashing for high-cardinality categorical variables.\n",
    "\n",
    "> Enoch's comment:\n",
    "\n",
    "22. **Lag Features**:\n",
    "\n",
    "    - Create lag features for time-series data (e.g., previous month's `PageValues`).\n",
    "\n",
    "> Enoch's comment:\n",
    "\n",
    "23. **Cyclic Features**:\n",
    "\n",
    "    - Encode cyclic features like `Month` using sine and cosine transformations.\n",
    "\n",
    "> Enoch's comment: I heard about this, can look more into this\n",
    "\n",
    "24. **Feature Pruning**:\n",
    "\n",
    "    - Remove highly correlated features to avoid multicollinearity.\n",
    "\n",
    "> Enoch's comment:\n",
    "\n",
    "25. **Synthetic Features**:\n",
    "    - Generate synthetic features using GANs or other generative models.\n",
    "\n",
    "> Enoch's comment: Something to consider in the future, considering the number of positive samples are quite low (15.5%)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from scipy.stats import zscore\n",
    "\n",
    "\n",
    "def scale_and_normalize(X, method='standard'):\n",
    "    if method == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    elif method == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method. Choose 'standard' or 'minmax'.\")\n",
    "    return scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "def encode_categorical(X):\n",
    "    ordinal_mapping = {\n",
    "        'Month': {\n",
    "            'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'June': 6,\n",
    "            'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12\n",
    "        }\n",
    "    }\n",
    "    for col, mapping in ordinal_mapping.items():\n",
    "        if col in X.columns:\n",
    "            X[col] = X[col].map(mapping)\n",
    "\n",
    "    nominal_columns = ['OperatingSystems', 'Browser',\n",
    "                       'Region', 'TrafficType', 'VisitorType']\n",
    "    X = pd.get_dummies(X, columns=nominal_columns)\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def bin_features(X, columns, bins, labels=None):\n",
    "    for col in columns:\n",
    "        X[col] = pd.cut(X[col], bins=bins, labels=labels)\n",
    "    return X\n",
    "\n",
    "\n",
    "def aggregate_features(X, columns, method='sum'):\n",
    "    if method == 'sum':\n",
    "        X['Aggregated'] = X[columns].sum(axis=1)\n",
    "    elif method == 'mean':\n",
    "        X['Aggregated'] = X[columns].mean(axis=1)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method. Choose 'sum' or 'mean'.\")\n",
    "    return X\n",
    "\n",
    "\n",
    "def select_features(X, y, k=10):\n",
    "    selector = SelectKBest(score_func=f_classif, k=k)\n",
    "    X_new = selector.fit_transform(X, y)\n",
    "    return X_new, selector.get_support()\n",
    "\n",
    "\n",
    "def remove_outliers(X, threshold=3):\n",
    "    return X[(zscore(X) < threshold).all(axis=1)]\n",
    "\n",
    "\n",
    "def convert_to_binary_flags(X, columns):\n",
    "    for col in columns:\n",
    "        X[col] = X[col].astype(int)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_get_results(model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Feb'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m y \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRevenue\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Step 1: Encode categorical variables\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m X_encoded \u001b[38;5;241m=\u001b[39m \u001b[43mscale_and_normalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[1;32m      9\u001b[0m     X_encoded, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     11\u001b[0m train_and_get_results(model, X_train, X_test, y_train, y_test)\n",
      "Cell \u001b[0;32mIn[15], line 13\u001b[0m, in \u001b[0;36mscale_and_normalize\u001b[0;34m(X, method)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid method. Choose \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstandard\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminmax\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/data/lib/python3.12/site-packages/sklearn/utils/_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    325\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/data/lib/python3.12/site-packages/sklearn/base.py:918\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    903\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    904\u001b[0m             (\n\u001b[1;32m    905\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m    914\u001b[0m         )\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/miniconda3/envs/data/lib/python3.12/site-packages/sklearn/preprocessing/_data.py:894\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/data/lib/python3.12/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/data/lib/python3.12/site-packages/sklearn/preprocessing/_data.py:930\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[1;32m    899\u001b[0m \n\u001b[1;32m    900\u001b[0m \u001b[38;5;124;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;124;03m    Fitted scaler.\u001b[39;00m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    929\u001b[0m first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 930\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    938\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    940\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/data/lib/python3.12/site-packages/sklearn/utils/validation.py:2944\u001b[0m, in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2942\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m   2943\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m-> 2944\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2945\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m   2946\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m~/miniconda3/envs/data/lib/python3.12/site-packages/sklearn/utils/validation.py:973\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pandas_requires_conversion:\n\u001b[1;32m    969\u001b[0m     \u001b[38;5;66;03m# pandas dataframe requires conversion earlier to handle extension dtypes with\u001b[39;00m\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;66;03m# nans\u001b[39;00m\n\u001b[1;32m    971\u001b[0m     \u001b[38;5;66;03m# Use the original dtype for conversion if dtype is None\u001b[39;00m\n\u001b[1;32m    972\u001b[0m     new_dtype \u001b[38;5;241m=\u001b[39m dtype_orig \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m dtype\n\u001b[0;32m--> 973\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;66;03m# Since we converted here, we do not need to convert again later\u001b[39;00m\n\u001b[1;32m    975\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/data/lib/python3.12/site-packages/pandas/core/generic.py:6643\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   6637\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   6638\u001b[0m         ser\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors) \u001b[38;5;28;01mfor\u001b[39;00m _, ser \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   6639\u001b[0m     ]\n\u001b[1;32m   6641\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6642\u001b[0m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[0;32m-> 6643\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6644\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\n\u001b[1;32m   6645\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/data/lib/python3.12/site-packages/pandas/core/internals/managers.py:430\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    428\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 430\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mastype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/data/lib/python3.12/site-packages/pandas/core/internals/managers.py:363\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 363\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[1;32m    366\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[0;32m~/miniconda3/envs/data/lib/python3.12/site-packages/pandas/core/internals/blocks.py:758\u001b[0m, in \u001b[0;36mBlock.astype\u001b[0;34m(self, dtype, copy, errors, using_cow, squeeze)\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan not squeeze with more than one column.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    756\u001b[0m     values \u001b[38;5;241m=\u001b[39m values[\u001b[38;5;241m0\u001b[39m, :]  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[0;32m--> 758\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_array_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    760\u001b[0m new_values \u001b[38;5;241m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[1;32m    762\u001b[0m refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/data/lib/python3.12/site-packages/pandas/core/dtypes/astype.py:237\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[0;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    234\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtype\u001b[38;5;241m.\u001b[39mnumpy_dtype\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 237\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;66;03m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/data/lib/python3.12/site-packages/pandas/core/dtypes/astype.py:182\u001b[0m, in \u001b[0;36mastype_array\u001b[0;34m(values, dtype, copy)\u001b[0m\n\u001b[1;32m    179\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 182\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43m_astype_nansafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, np\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/data/lib/python3.12/site-packages/pandas/core/dtypes/astype.py:133\u001b[0m, in \u001b[0;36m_astype_nansafe\u001b[0;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mor\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;66;03m# Explicit copy, or required since NumPy can't view from / to object.\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'Feb'"
     ]
    }
   ],
   "source": [
    "# Step 0: Prepare data\n",
    "X = data.drop(columns=['Revenue'])\n",
    "y = data['Revenue']\n",
    "\n",
    "# Step 1: Encode categorical variables\n",
    "X_encoded = encode_categorical(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "train_and_get_results(model, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'float' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m X_encoded \u001b[38;5;241m=\u001b[39m aggregate_features(X_encoded, columns_to_aggregate, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Step 5: Remove outliers\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m X_no_outliers \u001b[38;5;241m=\u001b[39m \u001b[43mremove_outliers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_encoded\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnumerical_columns\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Step 6: Select important features\u001b[39;00m\n\u001b[1;32m     21\u001b[0m X_selected, selected_features_mask \u001b[38;5;241m=\u001b[39m select_features(X_no_outliers, y, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[0;32mIn[15], line 57\u001b[0m, in \u001b[0;36mremove_outliers\u001b[0;34m(X, threshold)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mremove_outliers\u001b[39m(X, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X[(\u001b[43mzscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m<\u001b[39m threshold)\u001b[38;5;241m.\u001b[39mall(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n",
      "File \u001b[0;32m~/miniconda3/envs/data/lib/python3.12/site-packages/scipy/stats/_stats_py.py:2924\u001b[0m, in \u001b[0;36mzscore\u001b[0;34m(a, axis, ddof, nan_policy)\u001b[0m\n\u001b[1;32m   2842\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mzscore\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, ddof\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, nan_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpropagate\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m   2843\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2844\u001b[0m \u001b[38;5;124;03m    Compute the z score.\u001b[39;00m\n\u001b[1;32m   2845\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2922\u001b[0m \u001b[38;5;124;03m           [-0.91611681, -0.89090508,  1.4983032 ,  0.88731639, -0.5785977 ]])\u001b[39;00m\n\u001b[1;32m   2923\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2924\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mzmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mddof\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mddof\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnan_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnan_policy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/data/lib/python3.12/site-packages/scipy/stats/_stats_py.py:3088\u001b[0m, in \u001b[0;36mzmap\u001b[0;34m(scores, compare, axis, ddof, nan_policy)\u001b[0m\n\u001b[1;32m   3086\u001b[0m         isconst \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mapply_along_axis(_isconst, axis, a)\n\u001b[1;32m   3087\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3088\u001b[0m     mn \u001b[38;5;241m=\u001b[39m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3089\u001b[0m     std \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mstd(axis\u001b[38;5;241m=\u001b[39maxis, ddof\u001b[38;5;241m=\u001b[39mddof, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   3090\u001b[0m     \u001b[38;5;66;03m# The intent is to check whether all elements of `a` along `axis` are\u001b[39;00m\n\u001b[1;32m   3091\u001b[0m     \u001b[38;5;66;03m# identical. Due to finite precision arithmetic, comparing elements\u001b[39;00m\n\u001b[1;32m   3092\u001b[0m     \u001b[38;5;66;03m# against `mn` doesn't work. Previously, this compared elements to\u001b[39;00m\n\u001b[1;32m   3093\u001b[0m     \u001b[38;5;66;03m# `_first`, but that extracts the element at index 0 regardless of\u001b[39;00m\n\u001b[1;32m   3094\u001b[0m     \u001b[38;5;66;03m# whether it is masked. As a simple fix, compare against `min`.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/data/lib/python3.12/site-packages/numpy/_core/_methods.py:127\u001b[0m, in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m    124\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m mu\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    125\u001b[0m         is_float16_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mumr_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, mu\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _no_nep50_warning():\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'float' and 'str'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 2: Scale and normalize numerical features\n",
    "numerical_columns = X_encoded.select_dtypes(\n",
    "    include=['float64', 'int64']).columns\n",
    "X_scaled = scale_and_normalize(X_encoded[numerical_columns], method='standard')\n",
    "\n",
    "# Step 3: Bin numerical features\n",
    "columns_to_bin = ['Administrative_Duration',\n",
    "                  'Informational_Duration', 'ProductRelated_Duration']\n",
    "bins = [0, 10, 50, 100, 500]  # Example bin edges\n",
    "labels = ['Low', 'Medium', 'High', 'Very High']\n",
    "X_encoded = bin_features(X_encoded, columns_to_bin, bins, labels)\n",
    "\n",
    "# Step 4: Aggregate features\n",
    "columns_to_aggregate = ['Administrative', 'Informational', 'ProductRelated']\n",
    "X_encoded = aggregate_features(X_encoded, columns_to_aggregate, method='sum')\n",
    "\n",
    "# Step 5: Remove outliers\n",
    "X_no_outliers = remove_outliers(X_encoded[numerical_columns])\n",
    "\n",
    "# Step 6: Select important features\n",
    "X_selected, selected_features_mask = select_features(X_no_outliers, y, k=10)\n",
    "\n",
    "# Step 7: Convert boolean columns to binary flags\n",
    "boolean_columns = ['Weekend']\n",
    "X_encoded = convert_to_binary_flags(X_encoded, boolean_columns)\n",
    "\n",
    "# Now, X_encoded or X_selected can be used for training models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
